{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "# from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from textblob import Word\n",
    "pd.options.display.width = 0\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "pd.set_option('display.max_rows', 6000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "#pd.describe_option('display')\n",
    "# import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/nfs/home/vyasa/projects/proj_off/data_off/clarify/spanish_comorbidity/new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+'comorbidities_modified_witht_abbr.csv', sep=',', error_bad_lines=True, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IsValueReplaced'] = df['value']!=df['value_witht_abbr']\n",
    "cols_l = df.columns.tolist()\n",
    "df_new = df[cols_l[0:2]+cols_l[-1:]+cols_l[2:-1]]\n",
    "df = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['replacement'] = df['replacement'].str.replace(';', '')\n",
    "df['replacement'] = df['replacement'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_dict = {'Benign BIRADS2 breast nodule': 'Bening breast nodule (BI-RADS)',\n",
    " 'HT+ DA cavit': 'Hormone therapy',\n",
    " 'Bilateral hydrosadenitis': 'Hidradenitis suppurativa',\n",
    " 'SCASEST': 'Acute coronary syndrome',\n",
    " 'IAM': 'Acute Myocardial Infarction',\n",
    " 'Alcholic': 'Alcoholic',\n",
    " 'Hypoacucia': 'Hearing loss',\n",
    " 'sd IAVD': 'Right ventricle Acute myocardial infarction',\n",
    " 'Saphenous ischemia': 'Saphenous vein Ischemia'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'replacement':correction_dict}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Spanish Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_google = pd.read_csv(path+'comorbidities_spanish_english_google.csv', sep=',', error_bad_lines=True, encoding='utf-8')\n",
    "print (df_sp_google.shape)\n",
    "\n",
    "df_sp_google_abbr = pd.read_csv(path+'comorbidities_value_abbr_google.csv', sep=',', error_bad_lines=True, encoding='utf-8')\n",
    "print (df_sp_google.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_deep = pd.read_csv(path+'comorbidities_spanish_english_deepl.csv', sep=',', error_bad_lines=True, encoding='utf-8')\n",
    "print (df_sp_deep.shape)\n",
    "\n",
    "df_sp_deep_abbr = pd.read_csv(path+'comorbidities_value_abbr_deepl.csv', sep=',', error_bad_lines=True, encoding='utf-8')\n",
    "print (df_sp_deep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['google'] = df_sp_google['value']\n",
    "df['deepl'] = df_sp_deep['value']\n",
    "df['google_abbr'] = df_sp_google_abbr['value_witht_abbr_google']\n",
    "df['deepl_abbr'] = df_sp_deep_abbr['value_witht_abbr_deepl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['google'] = df['google'].str.strip()\n",
    "df['deepl'] = df['deepl'].str.strip()\n",
    "df['google_abbr'] = df['google_abbr'].str.strip()\n",
    "df['deepl_abbr'] = df['deepl_abbr'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_(text):\n",
    "    if type(text)==str:\n",
    "        if len(text)>0:\n",
    "            # return text[0:1].capitalize()+text[1:].lower()\n",
    "            return ' '.join([i[0:1].upper()+i[1:] if len(i)>1 else i for i in text.split()])\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df['replacement'] = df['replacement'].apply(capitalize_)\n",
    "df['google'] = df['google'].apply(capitalize_)\n",
    "df['deepl'] = df['deepl'].apply(capitalize_)\n",
    "df['google_abbr'] = df['google_abbr'].apply(capitalize_)\n",
    "df['deepl_abbr'] = df['deepl_abbr'].apply(capitalize_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and years\n",
    "def remove_stopwords_numbers(text):\n",
    "    return ' '.join([word for word in word_tokenize(str(text)) \\\n",
    "        if word not in all_stopwords and not word.isdigit()])\n",
    "\n",
    "df['google_withst_num'] = df['google'].apply(remove_stopwords_numbers)\n",
    "df['deepl_withst_num'] = df['deepl'].apply(remove_stopwords_numbers)\n",
    "df['google_abbr_withst_num'] = df['google_abbr'].apply(remove_stopwords_numbers)\n",
    "df['deepl_abbr_withst_num'] = df['deepl_abbr'].apply(remove_stopwords_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Abbreviations\n",
    "def find_in_english_dict(text):\n",
    "    for w in word_tokenize(text):\n",
    "        if Word(w.strip().lower()).spellcheck()[0][1]!=1.0 and len(re.findall(r\"[A-Z]{2,}\", w))>0 \\\n",
    "            and Word(w.strip().lower()).spellcheck()[0][0]!=w.strip().lower():\n",
    "            print (\"True\",w)\n",
    "          # print(w)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df['haveAbbr'] = (df['value_witht_abbr'].apply(lambda x : len(re.findall(r\"[A-Z]{2,}\", x))>0)) & \\\n",
    "    (df['google'].apply(find_in_english_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate no of stop words\n",
    "def stop_words_length(text):\n",
    "    # print (word_tokenize(str(text)))\n",
    "    return len([word for word in word_tokenize(str(text)) if word in all_stopwords])\n",
    "\n",
    "df['stop_words_google_len'] = df['google'].apply(stop_words_length)\n",
    "df['stop_words_deepl_len'] = df['deepl'].apply(stop_words_length)\n",
    "df['stop_words_google_abbr_len'] = df['google_abbr'].apply(stop_words_length)\n",
    "df['stop_words_deepl_abbr_len'] = df['deepl_abbr'].apply(stop_words_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+'spanish_google_deep_last.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['haveAbbr']==True].to_csv(path+'spanish_google_deep_last_index.csv', index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_data\n",
    "df_MEV = df.dropna(subset=['replacement'])\n",
    "df_MEV.to_csv(path+'train_MEV.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_MEV.reset_index().iloc[[60,67,132,160,163,444,446,462,467,478,507,511,573,675,677,721,834]]['replacement'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MEV.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Total_Time ({}(in Hrs) : {}(in Mins)\".format((time.time()-past_time)/3600.0, (time.time()-past_time)/60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0399d1cc73644b9c8e7484462cd4f9b3b32f61f571975f1cd01f67b06c060b9c6",
   "display_name": "Python 3.7.10 64-bit ('scispacy': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}